graph TD
    A["12.2 异步计算"] --> B["基本概念"]
    B --> B1["同步与异步"]
    B --> B2["阻塞与非阻塞"]
    B --> B3["并发与并行"]
    
    A --> C["异步编程"]
    C --> C1["协程"]
    C --> C2["Future/Promise"]
    C --> C3["async/await"]
    C --> C4["生成器"]
    
    A --> D["事件循环"]
    D --> D1["事件队列"]
    D --> D2["任务调度"]
    D --> D3["IO多路复用"]
    D --> D4["定时器管理"]
    
    A --> E["回调函数"]
    E --> E1["回调注册"]
    E --> E2["回调执行"]
    E --> E3["错误处理"]
    E --> E4["回调链"]
    
    A --> F["实现方法"]
    F --> F1["PyTorch实现"]
    F1 --> F1a["torch.cuda.Stream"]
    F1 --> F1b["torch.cuda.Event"]
    F --> F2["TensorFlow实现"]
    F2 --> F2a["tf.data.Dataset"]
    F2 --> F2b["tf.function"]
    
    A --> G["优化技巧"]
    G --> G1["任务并行"]
    G --> G2["数据预取"]
    G --> G3["流水线优化"]
    G --> G4["内存管理"]
    
    A --> H["应用场景"]
    H --> H1["深度学习训练"]
    H --> H2["数据加载"]
    H --> H3["模型推理"]
    H --> H4["分布式计算"]
    
    A --> I["常见问题"]
    I --> I1["死锁"]
    I --> I2["竞态条件"]
    I --> I3["内存泄漏"]
    I --> I4["调试困难"]
    
    A --> J["优势特点"]
    J --> J1["提高资源利用率"]
    J --> J2["减少等待时间"]
    J --> J3["提高吞吐量"]
    J --> J4["更好的可扩展性"] 