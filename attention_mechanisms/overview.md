graph TD
    A["第10章 注意力机制"] --> B["10.1 注意力机制"]
    A --> C["10.2 自注意力和位置编码"]
    A --> D["10.3 Transformer架构"]
    A --> E["10.4 BERT预训练模型"]
    A --> F["10.5 自然语言推断"]
    A --> G["10.6 微调BERT"]
    A --> H["10.7 自然语言处理：预训练"]
    
    B --> B1["基本概念"]
    B --> B2["注意力类型"]
    B --> B3["实现方法"]
    
    C --> C1["自注意力"]
    C --> C2["位置编码"]
    C --> C3["注意力计算"]
    
    D --> D1["编码器"]
    D --> D2["解码器"]
    D --> D3["核心组件"]
    
    E --> E1["模型架构"]
    E --> E2["预训练任务"]
    E --> E3["实现方法"]
    
    F --> F1["任务定义"]
    F --> F2["数据集"]
    F --> F3["模型架构"]
    
    G --> G1["微调策略"]
    G --> G2["任务特定头部"]
    G --> G3["实现方法"]
    
    H --> H1["预训练任务"]
    H --> H2["预训练模型"]
    H --> H3["实现方法"] 