graph TD
    A["8.17 Transformer架构"] --> B["编码器"]
    B --> B1["多头自注意力"]
    B --> B2["前馈网络"]
    B --> B3["残差连接"]
    
    A --> C["解码器"]
    C --> C1["掩码多头自注意力"]
    C --> C2["多头交叉注意力"]
    C --> C3["前馈网络"]
    
    A --> D["核心组件"]
    D --> D1["位置编码"]
    D --> D2["层归一化"]
    D --> D3["Dropout"]
    
    A --> E["实现方法"]
    E --> E1["PyTorch实现"]
    E1 --> E1a["nn.Transformer"]
    E1 --> E1b["自定义实现"]
    E --> E2["TensorFlow实现"]
    E2 --> E2a["tf.keras.layers.Transformer"]
    E2 --> E2b["自定义层"]
    
    A --> F["优化技巧"]
    F --> F1["学习率调度"]
    F --> F2["标签平滑"]
    F --> F3["梯度裁剪"]
    
    A --> G["应用场景"]
    G --> G1["机器翻译"]
    G --> G2["文本生成"]
    G --> G3["文本分类"]
    G --> G4["问答系统"]
    
    A --> H["变体模型"]
    H --> H1["BERT"]
    H --> H2["GPT"]
    H --> H3["T5"] 