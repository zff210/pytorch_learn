graph TD
    A["8.15 注意力机制"] --> B["基本概念"]
    B --> B1["查询向量"]
    B --> B2["键向量"]
    B --> B3["值向量"]
    
    A --> C["注意力类型"]
    C --> C1["点积注意力"]
    C --> C2["加性注意力"]
    C --> C3["缩放点积注意力"]
    
    A --> D["实现方法"]
    D --> D1["PyTorch实现"]
    D1 --> D1a["nn.MultiheadAttention"]
    D1 --> D1b["自定义实现"]
    D --> D2["TensorFlow实现"]
    D2 --> D2a["tf.keras.layers.MultiHeadAttention"]
    D2 --> D2b["自定义层"]
    
    A --> E["注意力变体"]
    E --> E1["自注意力"]
    E --> E2["交叉注意力"]
    E --> E3["多头注意力"]
    
    A --> F["优化技巧"]
    F --> F1["掩码机制"]
    F --> F2["位置编码"]
    F --> F3["残差连接"]
    
    A --> G["应用场景"]
    G --> G1["机器翻译"]
    G --> G2["文本摘要"]
    G --> G3["问答系统"]
    G --> G4["图像描述"]
    
    A --> H["优势特点"]
    H --> H1["长程依赖"]
    H --> H2["并行计算"]
    H --> H3["可解释性"] 