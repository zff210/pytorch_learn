graph TD
    A["8.23 自然语言处理：预训练模型"] --> B["模型类型"]
    B --> B1["自编码模型"]
    B1 --> B1a["BERT"]
    B1 --> B1b["RoBERTa"]
    B --> B2["自回归模型"]
    B2 --> B2a["GPT"]
    B2 --> B2b["GPT-2"]
    B --> B3["序列到序列模型"]
    B3 --> B3a["T5"]
    B3 --> B3b["BART"]
    
    A --> C["预训练任务"]
    C --> C1["掩码语言模型"]
    C --> C2["下一句预测"]
    C --> C3["语言模型"]
    
    A --> D["实现方法"]
    D --> D1["PyTorch实现"]
    D1 --> D1a["transformers库"]
    D1 --> D1b["自定义实现"]
    D --> D2["TensorFlow实现"]
    D2 --> D2a["transformers库"]
    D2 --> D2b["自定义层"]
    
    A --> E["微调策略"]
    E --> E1["全参数微调"]
    E --> E2["部分参数微调"]
    E --> E3["适配器微调"]
    
    A --> F["优化技巧"]
    F --> F1["学习率调度"]
    F --> F2["权重衰减"]
    F --> F3["梯度裁剪"]
    
    A --> G["应用场景"]
    G --> G1["文本分类"]
    G --> G2["问答系统"]
    G --> G3["文本生成"]
    G --> G4["机器翻译"]
    
    A --> H["评估方法"]
    H --> H1["困惑度"]
    H --> H2["下游任务评估"]
    H --> H3["零样本学习"] 